{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "a)A vocabulary of known words.\n",
    "b)A measure of the presence of known words.\n",
    "\n",
    "\n",
    "\n",
    "Drawbacks of using a Bag-of-Words (BoW) Model:\n",
    "\n",
    "\n",
    "\n",
    "1.If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.\n",
    "2.Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)\n",
    "3.We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize as st, word_tokenize as wt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer as ps, WordNetLemmatizer as wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "para='''An atom is the smallest unit of ordinary matter that forms a chemical element.\n",
    "\t\tEvery solid, liquid, gas, and plasma is composed of neutral or ionized atoms.\n",
    "\t\tAtoms are extremely small, typically around 100 picometers across. \n",
    "\t\tThey are so small that accurately predicting their behavior using classical physics—as \n",
    "\t\tif they were tennis balls, for example—is not possible due to quantum effects.\n",
    "\t\tEvery atom is composed of a nucleus and one or more electrons bound to the nucleus. \n",
    "\t\tThe nucleus is made of one or more protons and a number of neutrons. \n",
    "\t\tOnly the most common variety of hydrogen has no neutrons. \n",
    "\t\tMore than 99.94% of an atom's mass is in the nucleus. \n",
    "\t\tThe protons have a positive electric charge, the electrons have a negative electric charge, \n",
    "\t\tand the neutrons have no electric charge. If the number of protons and electrons are equal, \n",
    "\t\tthen the atom is electrically neutral. If an atom has more or fewer electrons than protons, \n",
    "\t\tthen it has an overall negative or positive charge, respectively – such atoms are called ions.\n",
    "\t\tThe electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. \n",
    "\t\tThe protons and neutrons in the nucleus are attracted to each other by the nuclear force. \n",
    "\t\tThis force is usually stronger than the electromagnetic force that repels the positively \n",
    "\t\tcharged protons from one another. Under certain circumstances, the repelling electromagnetic \n",
    "\t\tforce becomes stronger than the nuclear force. In this case, the nucleus splits and leaves \n",
    "\t\tbehind different elements. This is a form of nuclear decay.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ps=ps() #object creation porter stemmer\n",
    "wl=wl() #object creation word net lemmatizer\n",
    "sentences=st(para) #tokenizing to sentences\n",
    "corpus=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    rev=re.sub('[^a-zA-Z]',' ',sentences[i]) #everything except alphabets would be replaced by space\n",
    "    rev=rev.lower() \n",
    "    rev=rev.split() #splits them word wise into elements of a list\n",
    "    rev=[wl.lemmatize(word) for word in rev if word not in set(stopwords.words('english'))]\n",
    "    rev=' '.join(rev)\n",
    "    corpus.append(rev) #appending to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer #importing countervectorizer\n",
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(corpus).toarray() #transforming it to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF, which stands for term frequency — inverse document frequency, is a scoring measure widely used in information retrieval (IR) or summarization. TF-IDF is intended to reflect how relevant a term is in a given document.\n",
    "\n",
    ">The intuition behind it is that if a word occurs multiple times in a document, we should boost its relevance as it should be more meaningful than other words that appear fewer times (TF).\n",
    "TF=No Of Repeated Words in a Sentence/No of words in a sentence\n",
    "\n",
    ">At the same time, if a word occurs many times in a document but also along many other documents, maybe it is because this word is just a frequent word; not because it was relevant or meaningful (IDF).\n",
    "IDF=log(No of sentences/No of sentences containing words)\n",
    "\n",
    "TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "cv=tfidf() #object creation\n",
    "x=cv.fit_transform(corpus).toarray() #transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.40679695, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
